{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sudarshan-koirala/langchain-falcon-chainlit/blob/main/langchain_falcon.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE CASE:    CONNECTED\n",
    "#   TEXT  (YouTube Example = https://www.youtube.com/watch?v=gnyUUY8X-G4)   \n",
    "# + VIDEO (https://huggingface.co/learn/audio-course/chapter6/pre-trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install langchain huggingface_hub watermark\n",
    "%pip install transformers datasets soundfile speechbrain accelerate\n",
    "%pip install sentencepiece\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Falcon FutureTech Maverick Team\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.16.1\n",
      "\n",
      "langchain      : 0.0.324\n",
      "huggingface_hub: 0.17.3\n",
      "\n",
      "Compiler    : GCC 9.4.0\n",
      "OS          : Linux\n",
      "Release     : 6.2.0-1015-azure\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 2\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%load_ext watermark\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Falcon FutureTech Maverick Team\" -vmp langchain,huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your Huggingface access token from https://huggingface.co/settings/tokens ðŸ”‘\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "HUGGINGFACE_API_TOKEN = getpass()\n",
    "os.environ[\"HUGGINGFACE_API_TOKEN\"] = HUGGINGFACE_API_TOKEN   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use falcon-7b-instruct model from [Huggingface website](https://huggingface.co/tiiuae/falcon-7b-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/falcon-main/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspaces/falcon-main/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"tiiuae/falcon-7b-instruct\"\n",
    "llm = HuggingFaceHub(huggingfacehub_api_token=HUGGINGFACE_API_TOKEN, \n",
    "                     repo_id=repo_id, \n",
    "                     model_kwargs={\"temperature\":0.7, \"max_new_tokens\":700})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful AI assistant and provide the answer for the question asked politely.\n",
    "\n",
    "{question}\n",
    "Answer: Let's think step by step.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How to cook Pizza ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Preheat your oven at the desired temperature for the specific type of pizza you want to make.\n",
      "2. Roll out your dough on a baking sheet or pizza stone.\n",
      "3. Spread your favorite pizza sauce on the dough.\n",
      "4. Add your desired amount of cheese.\n",
      "5. Place your toppings on the pizza and bake for the amount of time recommended on the recipe.\n",
      "6. Remove the pizza from the oven and let it cool before slicing. \n",
      "7. Enjoy your delicious pizza!\n",
      "\n",
      "I hope this helps!\n"
     ]
    }
   ],
   "source": [
    "responseText = llm_chain.run(question)\n",
    "\n",
    "len(responseText)\n",
    "\n",
    "print(responseText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=responseText, return_tensors=\"pt\")\n",
    "\n",
    "print (inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s load such a speaker embedding from a dataset on the Hub. \n",
    "\n",
    "# the embeddings were obtained from the CMU ARCTIC dataset: \n",
    "# http://www.festvox.org/cmu_arctic/\n",
    "\n",
    "# using this script: \n",
    "# https://huggingface.co/mechanicalsea/speecht5-vc/blob/main/manifest/utils/prep_cmu_arctic_spkemb.py\n",
    "\n",
    "# but any X-Vector embedding should work.\n",
    "\n",
    "from datasets import load_dataset\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "len(embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The speaker embedding is a tensor of shape (1, 512). \n",
    "# This particular speaker embedding describes a female voice.\n",
    "import torch\n",
    "\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "print(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we already have enough inputs to generate a log mel spectrogram as an output, \n",
    "# you can do it like this:\n",
    "\n",
    "spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectrogram)\n",
    "\n",
    "# This outputs a tensor of shape (140, 80) containing a log mel spectrogram.\n",
    "\n",
    "# it may vary between runs as the speech decoder pre-net always applies dropout to the input sequence\n",
    "\n",
    "# This adds a bit of random variability to the generated speech\n",
    "\n",
    "# if we are looking to generate speech waveform, we need to specify a vocoder to use for the spectrogram to waveform conversion\n",
    "\n",
    "# Conveniently, ðŸ¤— Transformers offers a vocoder based on HiFi-GAN\n",
    "\n",
    "# HiFi-GAN is a state-of-the-art generative adversarial network (GAN) designed for high-fidelity speech synthesis. \n",
    "# It is capable of generating high-quality and realistic audio waveforms from spectrogram inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(speech, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
